{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Conv1D, MaxPool2D, Flatten, Dropout, GRU, CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from time import time\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import tensorflow as tf\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keijzer import *\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"} # Make sure the axis background of plots is white, this is usefull for the black theme in JupyterLab\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup multi GPU usage\n",
    "num_gpu = setup_multi_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = _\n",
    "path = path[:-10] # removes '\\\\notebooks' from the path string\n",
    "\n",
    "df = pd.read_csv(path+\"\\\\data\\\\house_data_processed.csv\", delimiter='\\t', parse_dates=['datetime'])\n",
    "df = df.set_index(['datetime']) \n",
    "\n",
    "magnitude = 1 # Take this from the 1. EDA & Feauture engineering notebook. It's the factor where gasPower has been scaled with to the power 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Prepare the data for the used model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime info to categorical\n",
    "Certain functions are able to use the Pandas categorical datatype, e.g. they don't require the feautures to be one-hot encoded.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_category = ['hour', 'dayofweek', 'season']\n",
    "data[columns_to_category] = data[columns_to_category].astype('category') # change datetypes to category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding // dummy variables\n",
    "One hot encode the categorical feautures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=columns_to_category) # One hot encoding the categories\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a copy of the target column\n",
    "This is because the `df_cnn_rnn_format()` function removes the target from the train dataset.  \n",
    "This copied column functions as the historical gasPower, just like ePower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gasPower_copy'] = data['gasPower']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, data to RNN format\n",
    "Setting the `look_back`, `test_size` and some other parameters.  \n",
    "Remember how the `df_to_lstm_rnn_format()` function splits the data into train & test and also has the ability to scale the X data per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 5*24 # D -> 5, H -> 5*24\n",
    "num_features = data.shape[1] - 1\n",
    "output_dim = 1\n",
    "train_size = 0.7\n",
    "\n",
    "X_train, y_train, X_test, y_test = df_to_cnn_rnn_format(df=data, train_size=train_size, look_back=look_back, target_column='gasPower', scale_X=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look of the train shape.  \n",
    "There are (number of samples, rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the train & test set target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(data.index, data['gasPower'], '.-', color='red', label='Original data', alpha=0.5)\n",
    "plt.xlabel('Datetime [-]', fontsize=20)\n",
    "plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % (magnitude), fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=14, rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.title('Mean gas usage of 54 houses', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('figures/available data.png', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The train & test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO: Fix te look_back time difference.`  \n",
    "For example, when loockback is 120 days, X_train length will be 4446 and y_train 4326...  \n",
    "y_train is 120 shorter than X_train because the first 120 y values cannot be predicted due there not being 120 historical X values...  \n",
    "So in the plots below there might be a 120 hour datetime difference with the actual datetime.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the X_train and X_test datetime values\n",
    "\"\"\"\n",
    "split_index = int(data.shape[0]*train_size)\n",
    "\n",
    "X_train_values = data[:split_index] # get the datetime values of X_train\n",
    "X_test_values = data[split_index:] # get the datetime values of X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_values.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_values.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporary fix for the datetime length difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_difference = len(X_train_values) - len(y_train)\n",
    "X_train_values = X_train_values[datetime_difference:] # Correct for datetime difference, this is a dirty way of doing it\n",
    "X_train_values.shape, y_train.shape\n",
    "\n",
    "datetime_difference = len(X_test_values) - len(y_test)\n",
    "X_test_values = X_test_values[datetime_difference:] # Correct for datetime difference, this is a dirty way of doing it\n",
    "X_test_values.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the plot\n",
    "\"\"\"\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(X_train_values.index, y_train, '.-', color='red', label='Train set', alpha=0.5)\n",
    "plt.plot(X_test_values.index, y_test, '.-', color='blue', label='Test set', alpha=0.5)\n",
    "\n",
    "plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % magnitude, fontsize=14)\n",
    "plt.xlabel('datetime [-]', fontsize=14) #TODO: set x values as actual dates\n",
    "\n",
    "plt.xticks(fontsize=14, rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.legend(loc='upper left', borderaxespad=0, frameon=False, fontsize=14, markerscale=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n",
    "`return_sequences` return the hidden state output for each input time step, for more info see [this article](https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/).  \n",
    "CuDNNLSTM is the CuDNN implementation of LSTM, these building blocks use GPU computation instead of CPU computation.  \n",
    "In general multiple smaller layers behind each other seemed to give better performance than one large layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(CuDNNLSTM(8, input_shape=(look_back, num_features), return_sequences=True, kernel_initializer='TruncatedNormal'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.2, seed=seed))\n",
    "\n",
    "model.add(CuDNNLSTM(8, return_sequences=True, kernel_initializer='TruncatedNormal'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.2, seed=seed))\n",
    "\n",
    "model.add(CuDNNLSTM(8, return_sequences=False, kernel_initializer='TruncatedNormal'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.2, seed=seed))\n",
    "\n",
    "model.add(Dense(32, kernel_initializer='TruncatedNormal'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.2, seed=seed))\n",
    "\n",
    "model.add(Dense(16, kernel_initializer='TruncatedNormal'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.2, seed=seed))\n",
    "\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "Also set some hyperparameters like the learning rate (lr), amount of epochs and the batch size (bs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Look back , 5\n",
    "nodes, 35\n",
    "\n",
    "More only makes the model more complex and harder/slower to train\n",
    "\"\"\"\n",
    "\n",
    "epochs = 200\n",
    "bs = 2**13\n",
    "lr = 1e-1\n",
    "print(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.05 0.9 0 True\n",
    "sgd = SGD(lr=0.5, momentum=0.9, decay=0, nesterov=True) # sgd in general yields better results, but needs a lot of tweeking and is slower\n",
    "adam = Adam(lr=lr)\n",
    "nadam = Nadam(lr=lr)\n",
    "\n",
    "# compile & fit\n",
    "model.compile(optimizer=adam, loss = ['mse'], metrics=[mape, smape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=5000)\n",
    "\n",
    "# This is used to save the best model, currently monitoring val_mape\n",
    "# checkpoint\n",
    "filepath=\"models\\\\LSTM.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_mape', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=bs, validation_data=(X_test, y_test),\n",
    "         verbose=1, callbacks=[PlotLossesKeras(), early_stopping_monitor, checkpoint])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the architecture\n",
    "model = load_model('models\\\\LSTM.best.hdf5', custom_objects={'smape': smape, \n",
    "                                                    'mape': mape}) # Gave an error when loading without 'custom_objects'.. fixed by https://github.com/keras-team/keras/issues/3911\n",
    "\n",
    "# Compile with the same settings as it has been saved with earlier\n",
    "model.compile(loss='mse', metrics=[mape, smape], optimizer=adam)\n",
    "\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_true = y_test.reshape(y_test.shape[0], 1)\n",
    "\n",
    "split_index = int(data.shape[0]*train_size)\n",
    "x = data[split_index:]\n",
    "\n",
    "datetime_difference = len(x) - len(y_true)\n",
    "x = x[datetime_difference:] # Correct for datetime difference, this is a dirty way of doing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(x.index, y_true, '.-', color='red', label='Real values', alpha=0.5)\n",
    "plt.plot(x.index, y_pred, '.-', color='blue', label='Predicted values', alpha=1)\n",
    "\n",
    "plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % magnitude, fontsize=14)\n",
    "plt.xlabel('datetime [-]', fontsize=14) #TODO: set x values as actual dates\n",
    "\n",
    "plt.xticks(fontsize=14, rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.legend(loc='upper left', borderaxespad=0, frameon=False, fontsize=14, markerscale=3)\n",
    "\n",
    "mse_result, mape_result, smape_result = model.evaluate(X_test, y_test)\n",
    "\n",
    "plt.title('LSTM result \\n MSE = %.2f \\n MAPE = %.1f [%%] \\n SMAPE = %.1f [%%]' % (mse_result, mape_result, smape_result), fontsize = 14)\n",
    "\n",
    "#plt.savefig('figures/Feedforward result hourly without dummy variables.png', dpi=1200)\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample these results to a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it a df to be able to downsample\n",
    "datetime = x.index\n",
    "print(datetime.shape)\n",
    "\n",
    "y_pred = y_pred.reshape(y_pred.shape[0])\n",
    "y_true = y_true.reshape(y_true.shape[0])\n",
    "\n",
    "results = pd.DataFrame(y_true, y_pred) # For some reason y_true becomes the index\n",
    "result = results.reset_index() # Ugly way to fix above problem\n",
    "result.columns = ['y_pred', 'y_true']\n",
    "\n",
    "result['datetime'] = datetime\n",
    "result = result.set_index(['datetime'])\n",
    "\n",
    "# Save the model results for later usage\n",
    "result.to_csv('models\\\\LSTM_predictions.csv')\n",
    "\n",
    "\n",
    "result = result.resample('D').sum() # Resample data\n",
    "\n",
    "result = result.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics over the result\n",
    "\n",
    "ytrue = result['y_true']\n",
    "ypred = result['y_pred']\n",
    "n = len(result)\n",
    "\n",
    "\n",
    "# Recalculated the metrics for the downsampled results\n",
    "mse_result = (1/n)*np.sum((ypred - ytrue)**2)\n",
    "mape_result = (100/n) * np.sum(np.abs((ytrue - ypred) / ypred))\n",
    "smape_result = (100/n) * np.sum( np.abs((ytrue - ypred)) / (np.abs(ytrue) + np.abs(ypred)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(result.index, result['y_true'], '.-', color='red', label='Real values', alpha=0.5, ms=10) # ms is markersize\n",
    "plt.plot(result.index, result['y_pred'], '.-', color='blue', label='Predicted values', ms=10)\n",
    "\n",
    "plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % magnitude, fontsize=14)\n",
    "plt.xlabel('datetime [-]', fontsize=14) #TODO: set x values as actual dates\n",
    "\n",
    "plt.xticks(fontsize=14, rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.legend(loc='upper left', borderaxespad=0, frameon=False, fontsize=14, markerscale=3)\n",
    "\n",
    "plt.title('LSTM result \\n MSE = %.2f \\n MAPE = %.1f [%%] \\n SMAPE = %.1f [%%]' % (mse_result, mape_result, smape_result), fontsize = 14)\n",
    "\n",
    "#plt.savefig('figures/LSTM result hourly resampled to daily by sum.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
